{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2399dc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-26 20:45:32.757526: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/cuda/include:/usr/lib/cuda/lib64:/home/irfanrah/catkin_ws/devel/lib:/opt/ros/noetic/lib\n",
      "2022-02-26 20:45:32.757546: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-02-26 20:45:36.267443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 20:45:36.267747: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/cuda/include:/usr/lib/cuda/lib64:/home/irfanrah/catkin_ws/devel/lib:/opt/ros/noetic/lib\n",
      "2022-02-26 20:45:36.267812: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/cuda/include:/usr/lib/cuda/lib64:/home/irfanrah/catkin_ws/devel/lib:/opt/ros/noetic/lib\n",
      "2022-02-26 20:45:36.267866: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/cuda/include:/usr/lib/cuda/lib64:/home/irfanrah/catkin_ws/devel/lib:/opt/ros/noetic/lib\n",
      "2022-02-26 20:45:36.269094: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/cuda/include:/usr/lib/cuda/lib64:/home/irfanrah/catkin_ws/devel/lib:/opt/ros/noetic/lib\n",
      "2022-02-26 20:45:36.269144: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/cuda/include:/usr/lib/cuda/lib64:/home/irfanrah/catkin_ws/devel/lib:/opt/ros/noetic/lib\n",
      "2022-02-26 20:45:36.269191: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/cuda/include:/usr/lib/cuda/lib64:/home/irfanrah/catkin_ws/devel/lib:/opt/ros/noetic/lib\n",
      "2022-02-26 20:45:36.269199: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-02-26 20:45:36.269397: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "625/625 [==============================] - 33s 50ms/step - loss: 0.6344 - accuracy: 0.6173 - val_loss: 0.5268 - val_accuracy: 0.7442 - lr: 0.0010\n",
      "Epoch 2/30\n",
      "625/625 [==============================] - 31s 49ms/step - loss: 0.5490 - accuracy: 0.6875 - val_loss: 0.5415 - val_accuracy: 0.6742 - lr: 0.0010\n",
      "Epoch 3/30\n",
      "625/625 [==============================] - 30s 49ms/step - loss: 0.5105 - accuracy: 0.7483 - val_loss: 0.4970 - val_accuracy: 0.7722 - lr: 0.0010\n",
      "Epoch 4/30\n",
      "625/625 [==============================] - 31s 49ms/step - loss: 0.4621 - accuracy: 0.7925 - val_loss: 0.4474 - val_accuracy: 0.8018 - lr: 0.0010\n",
      "Epoch 5/30\n",
      "625/625 [==============================] - 30s 49ms/step - loss: 0.4393 - accuracy: 0.8051 - val_loss: 0.4342 - val_accuracy: 0.8013 - lr: 0.0010\n",
      "Epoch 6/30\n",
      "625/625 [==============================] - 31s 49ms/step - loss: 0.4127 - accuracy: 0.8185 - val_loss: 0.4444 - val_accuracy: 0.8006 - lr: 0.0010\n",
      "Epoch 7/30\n",
      "624/625 [============================>.] - ETA: 0s - loss: 0.4070 - accuracy: 0.8229"
     ]
    }
   ],
   "source": [
    "# ======================================================================\n",
    "# There are 5 questions in this exam with increasing difficulty from 1-5.\n",
    "# Please note that the weight of the grade for the question is relative\n",
    "# to its difficulty. So your Category 1 question will score significantly\n",
    "# less than your Category 5 question.\n",
    "#\n",
    "# Don't use lambda layers in your model.\n",
    "# You do not need them to solve the question.\n",
    "# Lambda layers are not supported by the grading infrastructure.\n",
    "#\n",
    "# You must use the Submit and Test button to submit your model\n",
    "# at least once in this category before you finally submit your exam,\n",
    "# otherwise you will score zero for this category.\n",
    "# ======================================================================\n",
    "#\n",
    "# NLP QUESTION\n",
    "#\n",
    "# Build and train a classifier for the sarcasm dataset.\n",
    "# The classifier should have a final layer with 1 neuron activated by sigmoid as shown.\n",
    "# It will be tested against a number of sentences that the network hasn't previously seen\n",
    "# and you will be scored on whether sarcasm was correctly detected in those sentences.\n",
    "\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import urllib\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def solution_model():\n",
    "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json'\n",
    "    urllib.request.urlretrieve(url, 'sarcasm.json')\n",
    "\n",
    "    # DO NOT CHANGE THIS CODE OR THE TESTS MAY NOT WORK\n",
    "    vocab_size = 1000\n",
    "    embedding_dim = 16\n",
    "    max_length = 120\n",
    "    trunc_type='post'\n",
    "    padding_type='post'\n",
    "    oov_tok = \"<OOV>\"\n",
    "    training_size = 20000\n",
    "\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    # YOUR CODE HERE\n",
    "    df = pd.read_json('sarcasm.json')\n",
    "    sentences = df['headline']\n",
    "    labels = df['is_sarcastic']\n",
    "    \n",
    "    training_sentences = sentences[:training_size]\n",
    "    training_labels = labels[:training_size]\n",
    "    \n",
    "    testing_sentences = sentences[training_size:]\n",
    "    testing_labels = labels[training_size:]\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words = vocab_size , oov_token=oov_tok)\n",
    "    tokenizer.fit_on_texts(training_sentences)\n",
    "    word_index = tokenizer.word_index\n",
    "    \n",
    "    train_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "    train_padded = pad_sequences(train_sequences, maxlen=max_length, truncating=trunc_type, padding=padding_type)\n",
    "\n",
    "    validation_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "    validation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding=padding_type,\n",
    "                                      truncating=trunc_type)\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences = True)),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=3, min_lr=0.001)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    num_epochs = 30\n",
    "    model.fit(\n",
    "        train_padded,\n",
    "        training_labels,\n",
    "        validation_data=(validation_padded, testing_labels),\n",
    "        verbose=1,\n",
    "        epochs=num_epochs, callbacks=[reduce_lr]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# Note that you'll need to save your model as a .h5 like this.\n",
    "# When you press the Submit and Test button, your saved .h5 model will\n",
    "# be sent to the testing infrastructure for scoring\n",
    "# and the score will be returned to you.\n",
    "if __name__ == '__main__':\n",
    "    model = solution_model()\n",
    "    model.save(\"mymodel4.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80f4c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import urllib\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json'\n",
    "urllib.request.urlretrieve(url, 'sarcasm.json')\n",
    "df = pd.read_json('sarcasm.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a351ec61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2300e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
